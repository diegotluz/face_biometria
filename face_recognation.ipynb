{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegotluz/face_biometria/blob/main/face_recognation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nova seção"
      ],
      "metadata": {
        "id": "j-8RCING0LhV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Zt3L4fV8-3qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd5db65-ce4a-42bc-d6f3-2c4894ffb2fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.20)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.24)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "#@title Instalação de dependências\n",
        "!pip install mediapipe scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_FeLqmhK9hzf"
      },
      "outputs": [],
      "source": [
        "#@title Importações necessárias\n",
        "from IPython.display import display, Javascript\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import os\n",
        "import base64\n",
        "from google.colab.output import eval_js\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configurações globais\n",
        "KNOWN_FACES_PATH = \"./known_faces\"\n",
        "DATABASE_PATH = \"face_database.pkl\"\n",
        "FPS = 30\n",
        "SIMILARITY_THRESHOLD = 0.75\n",
        "MIN_BLINKS = 3"
      ],
      "metadata": {
        "id": "JxOYO3hK03op"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Classe Principal do Sistema Biométrico\n",
        "class FaceBiometricSystem:\n",
        "    def __init__(self):\n",
        "        self.LANDMARK_INDICES = [33, 133, 362, 263, 61, 291, 4, 168, 197, 195]\n",
        "        self.EYE_INDICES = {\n",
        "            'left': [362, 385, 387, 263, 373, 380],\n",
        "            'right': [33, 160, 158, 133, 153, 144]\n",
        "        }\n",
        "\n",
        "        self.face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
        "            static_image_mode=True,\n",
        "            max_num_faces=1,\n",
        "            refine_landmarks=True,\n",
        "            min_detection_confidence=0.7,\n",
        "            min_tracking_confidence=0.7\n",
        "        )\n",
        "\n",
        "        self.database = self._initialize_database()\n",
        "        self._validate_database()\n",
        "        self.scaler = StandardScaler()\n",
        "        self.clf = self._train_model()\n",
        "        self.blink_counter = 0\n",
        "        self.last_blink_time = time.time()\n",
        "\n",
        "    #@title Métodos de Gerenciamento de Banco de Dados\n",
        "    def _initialize_database(self):\n",
        "        if os.path.exists(DATABASE_PATH):\n",
        "            return self._load_database()\n",
        "        return self._create_database()\n",
        "\n",
        "    def _load_database(self):\n",
        "        try:\n",
        "            with open(DATABASE_PATH, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        except:\n",
        "            return self._create_database()\n",
        "\n",
        "    def _save_database(self, database):\n",
        "        with open(DATABASE_PATH, 'wb') as f:\n",
        "            pickle.dump(database, f)\n",
        "\n",
        "\n",
        "    def _create_database(self):\n",
        "        database = {}\n",
        "        for person in os.listdir(KNOWN_FACES_PATH):\n",
        "            person_dir = os.path.join(KNOWN_FACES_PATH, person)\n",
        "            if os.path.isdir(person_dir):\n",
        "                embeddings = []\n",
        "                for img_file in os.listdir(person_dir):\n",
        "                    img_path = os.path.join(person_dir, img_file)\n",
        "                    img = cv2.imread(img_path)\n",
        "                    if img is not None:\n",
        "                        emb = self._extract_embedding(img)\n",
        "                        if emb is not None:\n",
        "                            embeddings.append(emb)\n",
        "                if embeddings:\n",
        "                    database[person] = {\n",
        "                        'embeddings': embeddings,\n",
        "                        'avg_embedding': np.mean(embeddings, axis=0),\n",
        "                        'samples': len(embeddings)\n",
        "                    }\n",
        "        self._save_database(database)\n",
        "        return database\n",
        "\n",
        "    #@title Validação de Dados\n",
        "    def _validate_database(self):\n",
        "        if len(self.database) < 2:\n",
        "            raise ValueError(\"Cadastre pelo menos 2 pessoas no diretório 'known_faces'\")\n",
        "\n",
        "        for person, data in self.database.items():\n",
        "            if len(data['embeddings']) < 3:\n",
        "                raise ValueError(f\"{person} precisa de pelo menos 3 fotos para treinamento\")\n",
        "\n",
        "    #@title Processamento Facial\n",
        "    def _extract_embedding(self, image):\n",
        "        image = cv2.resize(image, (320, 240))\n",
        "        results = self.face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        if results.multi_face_landmarks:\n",
        "            return self._process_landmarks(results.multi_face_landmarks[0].landmark)\n",
        "        return None\n",
        "\n",
        "    def _process_landmarks(self, landmarks):\n",
        "        return np.array([(landmarks[i].x, landmarks[i].y) for i in self.LANDMARK_INDICES]).flatten()\n",
        "\n",
        "    #@title Treinamento do Modelo\n",
        "    def _train_model(self):\n",
        "        X = []\n",
        "        y = []\n",
        "        persons = list(self.database.keys())\n",
        "\n",
        "        # Geração de pares positivos\n",
        "        for person in persons:\n",
        "            embeddings = self.database[person]['embeddings']\n",
        "            for i in range(len(embeddings)):\n",
        "                for j in range(i+1, len(embeddings)):\n",
        "                    X.append(self._hybrid_metric(embeddings[i], embeddings[j]))\n",
        "                    y.append(1)\n",
        "\n",
        "        # Geração de pares negativos\n",
        "        for i in range(len(persons)):\n",
        "            for j in range(i+1, len(persons)):\n",
        "                emb1 = self.database[persons[i]]['embeddings']\n",
        "                emb2 = self.database[persons[j]]['embeddings']\n",
        "                for e1 in emb1[:3]:\n",
        "                    for e2 in emb2[:3]:\n",
        "                        X.append(self._hybrid_metric(e1, e2))\n",
        "                        y.append(0)\n",
        "\n",
        "        X = np.array(X).reshape(-1, 1)\n",
        "        self.scaler.fit(X)\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        return GradientBoostingClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=3,\n",
        "            subsample=0.8,\n",
        "            random_state=42\n",
        "        ).fit(X_scaled, y)\n",
        "\n",
        "    #@title Autenticação e Verificação\n",
        "    def authenticate(self, frame):\n",
        "        results = self.face_mesh.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        if not results.multi_face_landmarks:\n",
        "            return \"Desconhecido\", 0.0, False\n",
        "\n",
        "        landmarks = results.multi_face_landmarks[0].landmark\n",
        "        emb = self._process_landmarks(landmarks)\n",
        "\n",
        "        best_match = \"Desconhecido\"\n",
        "        best_score = 0.0\n",
        "\n",
        "        for person, data in self.database.items():\n",
        "            for db_emb in data['embeddings']:\n",
        "                features = self._hybrid_metric(emb, db_emb)\n",
        "                features = np.array([features]).reshape(-1, 1)\n",
        "                features = self.scaler.transform(features)\n",
        "                score = self.clf.predict_proba(features)[0][1]\n",
        "                if score > best_score and score > SIMILARITY_THRESHOLD:\n",
        "                    best_match = person\n",
        "                    best_score = score\n",
        "\n",
        "        liveness = self._detect_liveness(landmarks)\n",
        "        return best_match, best_score, liveness\n",
        "\n",
        "    #@title Métodos Auxiliares\n",
        "    def _hybrid_metric(self, emb1, emb2):\n",
        "        return np.linalg.norm(emb1 - emb2)\n",
        "\n",
        "    def _eye_aspect_ratio(self, landmarks, eye_type):\n",
        "        points = [(landmarks[i].x, landmarks[i].y) for i in self.EYE_INDICES[eye_type]]\n",
        "        vertical = np.linalg.norm(np.subtract(points[1], points[5])) + np.linalg.norm(np.subtract(points[2], points[4]))\n",
        "        horizontal = np.linalg.norm(np.subtract(points[0], points[3]))\n",
        "        return vertical / (2.0 * horizontal + 1e-6)\n",
        "\n",
        "    def _detect_liveness(self, landmarks):\n",
        "        left_ear = self._eye_aspect_ratio(landmarks, 'left')\n",
        "        right_ear = self._eye_aspect_ratio(landmarks, 'right')\n",
        "        ear = (left_ear + right_ear) / 2.0\n",
        "\n",
        "        current_time = time.time()\n",
        "        if ear < 0.21:\n",
        "            if (current_time - self.last_blink_time) > 0.3:\n",
        "                self.blink_counter += 1\n",
        "                self.last_blink_time = current_time\n",
        "        else:\n",
        "            if (current_time - self.last_blink_time) > 2.0:\n",
        "                self.blink_counter = 0  # Reset counter if no blinks for 2 seconds\n",
        "\n",
        "        return self.blink_counter >= MIN_BLINKS\n",
        "\n",
        "    #@title Interface de Vídeo\n",
        "    def _video_stream(self):\n",
        "        js = Javascript('''\n",
        "            var video;\n",
        "            var div = null;\n",
        "            var stream;\n",
        "            var captureCanvas;\n",
        "            var imgElement;\n",
        "            var labelElement;\n",
        "\n",
        "            var pendingResolve = null;\n",
        "            var shutdown = false;\n",
        "\n",
        "            function removeDom() {\n",
        "                if(stream) stream.getVideoTracks()[0].stop();\n",
        "                if(video) video.remove();\n",
        "                if(div) div.remove();\n",
        "                video = null;\n",
        "                div = null;\n",
        "                stream = null;\n",
        "                imgElement = null;\n",
        "                captureCanvas = null;\n",
        "                labelElement = null;\n",
        "            }\n",
        "\n",
        "            function onAnimationFrame() {\n",
        "                if (!shutdown) {\n",
        "                    window.requestAnimationFrame(onAnimationFrame);\n",
        "                }\n",
        "                if (pendingResolve) {\n",
        "                    captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "                    var result = shutdown ? \"\" : captureCanvas.toDataURL('image/jpeg', 0.8);\n",
        "                    pendingResolve(result);\n",
        "                    pendingResolve = null;\n",
        "                }\n",
        "            }\n",
        "\n",
        "            async function createDom() {\n",
        "                if (div !== null) return stream;\n",
        "\n",
        "                div = document.createElement('div');\n",
        "                div.style.border = '2px solid black';\n",
        "                div.style.padding = '3px';\n",
        "                div.style.width = '100%';\n",
        "                div.style.maxWidth = '600px';\n",
        "                document.body.appendChild(div);\n",
        "\n",
        "                const modelOut = document.createElement('div');\n",
        "                modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "                labelElement = document.createElement('span');\n",
        "                labelElement.innerText = 'Inicializando...';\n",
        "                labelElement.style.fontWeight = 'bold';\n",
        "                modelOut.appendChild(labelElement);\n",
        "                div.appendChild(modelOut);\n",
        "\n",
        "                video = document.createElement('video');\n",
        "                video.style.display = 'block';\n",
        "                video.width = div.clientWidth - 6;\n",
        "                video.setAttribute('playsinline', '');\n",
        "                video.onclick = () => shutdown = true;\n",
        "                stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"user\"}});\n",
        "                div.appendChild(video);\n",
        "\n",
        "                imgElement = document.createElement('img');\n",
        "                imgElement.style.position = 'absolute';\n",
        "                imgElement.style.zIndex = 1;\n",
        "                imgElement.onclick = () => shutdown = true;\n",
        "                div.appendChild(imgElement);\n",
        "\n",
        "                const instruction = document.createElement('div');\n",
        "                instruction.innerHTML = '<span style=\"color: red; font-weight: bold;\">Clique na tela ou pressione Q para sair</span>';\n",
        "                div.appendChild(instruction);\n",
        "                instruction.onclick = () => shutdown = true;\n",
        "\n",
        "                video.srcObject = stream;\n",
        "                await video.play();\n",
        "\n",
        "                captureCanvas = document.createElement('canvas');\n",
        "                captureCanvas.width = 640;\n",
        "                captureCanvas.height = 480;\n",
        "                window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "                return stream;\n",
        "            }\n",
        "\n",
        "            async function stream_frame(label, imgData) {\n",
        "                if (shutdown) {\n",
        "                    removeDom();\n",
        "                    return '';\n",
        "                }\n",
        "\n",
        "                await createDom();\n",
        "                labelElement.innerHTML = label;\n",
        "\n",
        "                if (imgData) {\n",
        "                    var videoRect = video.getClientRects()[0];\n",
        "                    imgElement.style.top = videoRect.top + \"px\";\n",
        "                    imgElement.style.left = videoRect.left + \"px\";\n",
        "                    imgElement.style.width = videoRect.width + \"px\";\n",
        "                    imgElement.style.height = videoRect.height + \"px\";\n",
        "                    imgElement.src = imgData;\n",
        "                }\n",
        "\n",
        "                var result = await new Promise(resolve => pendingResolve = resolve);\n",
        "                return {'img': result};\n",
        "            }\n",
        "        ''')\n",
        "        display(js)\n",
        "\n",
        "    #@title Execução Principal\n",
        "    def run(self):\n",
        "        self._video_stream()\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                frame_data = eval_js('stream_frame(\"\", \"\")')\n",
        "                frame = cv2.imdecode(np.frombuffer(\n",
        "                    base64.b64decode(frame_data['img'].split(',')[1]),\n",
        "                    dtype=np.uint8), 1)\n",
        "\n",
        "                # Processar frame e desenhar retângulo\n",
        "                results = self.face_mesh.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "                if results.multi_face_landmarks:\n",
        "                    landmarks = results.multi_face_landmarks[0].landmark\n",
        "                    x_coords = [landmarks[i].x for i in self.LANDMARK_INDICES]\n",
        "                    y_coords = [landmarks[i].y for i in self.LANDMARK_INDICES]\n",
        "\n",
        "                    x_min, x_max = int(min(x_coords)*frame.shape[1]), int(max(x_coords)*frame.shape[1])\n",
        "                    y_min, y_max = int(min(y_coords)*frame.shape[0]), int(max(y_coords)*frame.shape[0])\n",
        "\n",
        "                    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "\n",
        "                    # Converter frame para exibição\n",
        "                    _, img_encoded = cv2.imencode('.jpg', frame)\n",
        "                    img_base64 = base64.b64encode(img_encoded).decode('utf-8')\n",
        "\n",
        "                    name, score, liveness = self.authenticate(frame)\n",
        "                    status = f\"Usuário: {name} | Confiança: {score:.2f} | Blinks: {self.blink_counter}\"\n",
        "                    eval_js(f'stream_frame(\"{status}\", \"data:image/jpeg;base64,{img_base64}\")')\n",
        "\n",
        "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                    eval_js('shutdown = true')\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Erro: {str(e)}\")\n",
        "                break"
      ],
      "metadata": {
        "id": "WFtPIkzL08rG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inicialização do Sistema\n",
        "if __name__ == \"__main__\":\n",
        "    if os.path.exists(DATABASE_PATH):\n",
        "        os.remove(DATABASE_PATH)\n",
        "    system = FaceBiometricSystem()\n",
        "    system.run()"
      ],
      "metadata": {
        "id": "85CvPDty1Ehq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhQt8FA6LEGCICerTrNa2L",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}